{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cbb7fRy-eyr"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sNDnxE2-pwE"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxChR1Rk-umf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uBTqR3nacj0e",
    "outputId": "4c0bd183-e424-429a-9fba-ceb841c06888"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG3FQEch-yuA"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4zq8Mza_D9O"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read excel dataset file:\n",
    "!pip install xlrd #I installed it before using Anaconda \"pip install xlrd\" or \"conda install xlrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9CV13Co_HHM"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('Folds5x2_pp.xlsx')\n",
    "\n",
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.96</td>\n",
       "      <td>41.76</td>\n",
       "      <td>1024.07</td>\n",
       "      <td>73.17</td>\n",
       "      <td>463.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.18</td>\n",
       "      <td>62.96</td>\n",
       "      <td>1020.04</td>\n",
       "      <td>59.08</td>\n",
       "      <td>444.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.11</td>\n",
       "      <td>39.40</td>\n",
       "      <td>1012.16</td>\n",
       "      <td>92.14</td>\n",
       "      <td>488.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.86</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1010.24</td>\n",
       "      <td>76.64</td>\n",
       "      <td>446.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.82</td>\n",
       "      <td>37.50</td>\n",
       "      <td>1009.23</td>\n",
       "      <td>96.62</td>\n",
       "      <td>473.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>16.65</td>\n",
       "      <td>49.69</td>\n",
       "      <td>1014.01</td>\n",
       "      <td>91.00</td>\n",
       "      <td>460.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>13.19</td>\n",
       "      <td>39.18</td>\n",
       "      <td>1023.67</td>\n",
       "      <td>66.78</td>\n",
       "      <td>469.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>31.32</td>\n",
       "      <td>74.33</td>\n",
       "      <td>1012.92</td>\n",
       "      <td>36.48</td>\n",
       "      <td>429.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9566</th>\n",
       "      <td>24.48</td>\n",
       "      <td>69.45</td>\n",
       "      <td>1013.86</td>\n",
       "      <td>62.39</td>\n",
       "      <td>435.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9567</th>\n",
       "      <td>21.60</td>\n",
       "      <td>62.52</td>\n",
       "      <td>1017.23</td>\n",
       "      <td>67.87</td>\n",
       "      <td>453.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9568 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AT      V       AP     RH      PE\n",
       "0     14.96  41.76  1024.07  73.17  463.26\n",
       "1     25.18  62.96  1020.04  59.08  444.37\n",
       "2      5.11  39.40  1012.16  92.14  488.56\n",
       "3     20.86  57.32  1010.24  76.64  446.48\n",
       "4     10.82  37.50  1009.23  96.62  473.90\n",
       "...     ...    ...      ...    ...     ...\n",
       "9563  16.65  49.69  1014.01  91.00  460.03\n",
       "9564  13.19  39.18  1023.67  66.78  469.62\n",
       "9565  31.32  74.33  1012.92  36.48  429.57\n",
       "9566  24.48  69.45  1013.86  62.39  435.74\n",
       "9567  21.60  62.52  1017.23  67.87  453.28\n",
       "\n",
       "[9568 rows x 5 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14.96,   41.76, 1024.07,   73.17],\n",
       "       [  25.18,   62.96, 1020.04,   59.08],\n",
       "       [   5.11,   39.4 , 1012.16,   92.14],\n",
       "       ...,\n",
       "       [  31.32,   74.33, 1012.92,   36.48],\n",
       "       [  24.48,   69.45, 1013.86,   62.39],\n",
       "       [  21.6 ,   62.52, 1017.23,   67.87]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([463.26, 444.37, 488.56, ..., 429.57, 435.74, 453.28])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VC6omXel_Up0"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5edeb2r_agx"
   },
   "outputs": [],
   "source": [
    "#There is no missing value in the dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #But Hadeline didn't do it! I don't know why, since he mentioned it is compulsory in ANN problems to do feature scaling!\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mSLlAT9_eyI"
   },
   "source": [
    "## Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsBULd_f_wLY"
   },
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6Hd97Ls__Nz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "ann = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iitAFJS_ABUn"
   },
   "source": [
    "### Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksO_Vv40AHix"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "ann.add(Dense(units = 6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lb4kK_wAKbs"
   },
   "source": [
    "### Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2357OqEAQOQ"
   },
   "outputs": [],
   "source": [
    "ann.add(Dense(units = 6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwMOmKb3AdBY"
   },
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFATpzsUAkLL"
   },
   "outputs": [],
   "source": [
    "ann.add(Dense(units = 1)) #in the \"Classification\" tasks we used \"Sigmoid\" or \"Softmax\" for the activation function of the output layer. In the Regression tasks, we leave \"activation\" blank! So it goes to the default of \"None\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fq7e4fF6A1yy"
   },
   "source": [
    "## Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDeylAs2An25"
   },
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pesgbWlCAtB4"
   },
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy']) #In ANN Regression tasks (Actually in ALL Regression tasks :DD) we use \"Loss = 'mean_squared_error'\".\n",
    "#I added \"metrics = ['accuracy']\" myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjVuiybYOo7r"
   },
   "source": [
    "### Training the ANN model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stop = EarlyStopping(monitor = 'val_loss', patience = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c_vV-tiiA5zn",
    "outputId": "4a2b6ee6-ed75-4698-9069-b250e613803f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7654 samples, validate on 1914 samples\n",
      "Epoch 1/100\n",
      "7654/7654 [==============================] - 1s 118us/sample - loss: 204522.2999 - acc: 0.0000e+00 - val_loss: 201231.9195 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "7654/7654 [==============================] - 1s 80us/sample - loss: 191954.5775 - acc: 0.0000e+00 - val_loss: 178638.1907 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "7654/7654 [==============================] - 1s 84us/sample - loss: 157561.6239 - acc: 0.0000e+00 - val_loss: 132507.8812 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "7654/7654 [==============================] - 1s 82us/sample - loss: 106086.2727 - acc: 0.0000e+00 - val_loss: 79330.1972 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 59667.1140 - acc: 0.0000e+00 - val_loss: 42471.2622 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "7654/7654 [==============================] - 1s 93us/sample - loss: 32704.1609 - acc: 0.0000e+00 - val_loss: 24439.6546 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "7654/7654 [==============================] - 1s 100us/sample - loss: 20169.8951 - acc: 0.0000e+00 - val_loss: 16188.8455 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "7654/7654 [==============================] - 1s 111us/sample - loss: 14441.4634 - acc: 0.0000e+00 - val_loss: 12137.7474 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "7654/7654 [==============================] - 1s 107us/sample - loss: 11257.3178 - acc: 0.0000e+00 - val_loss: 9483.9520 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "7654/7654 [==============================] - 1s 106us/sample - loss: 8815.8521 - acc: 0.0000e+00 - val_loss: 7289.8447 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "7654/7654 [==============================] - 1s 82us/sample - loss: 6712.7657 - acc: 0.0000e+00 - val_loss: 5408.2693 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "7654/7654 [==============================] - 1s 83us/sample - loss: 4931.3590 - acc: 0.0000e+00 - val_loss: 3902.6594 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "7654/7654 [==============================] - 1s 87us/sample - loss: 3528.5432 - acc: 0.0000e+00 - val_loss: 2776.5252 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "7654/7654 [==============================] - 1s 84us/sample - loss: 2518.5535 - acc: 0.0000e+00 - val_loss: 1998.7501 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 1821.5209 - acc: 0.0000e+00 - val_loss: 1462.8154 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "7654/7654 [==============================] - 1s 84us/sample - loss: 1324.2504 - acc: 0.0000e+00 - val_loss: 1073.4623 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 964.3442 - acc: 0.0000e+00 - val_loss: 791.8042 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "7654/7654 [==============================] - 1s 93us/sample - loss: 704.9618 - acc: 0.0000e+00 - val_loss: 586.8268 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "7654/7654 [==============================] - 1s 87us/sample - loss: 516.6361 - acc: 0.0000e+00 - val_loss: 435.1767 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "7654/7654 [==============================] - 1s 77us/sample - loss: 379.9784 - acc: 0.0000e+00 - val_loss: 321.5029 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "7654/7654 [==============================] - 1s 68us/sample - loss: 279.8020 - acc: 0.0000e+00 - val_loss: 237.3922 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "7654/7654 [==============================] - 1s 74us/sample - loss: 208.4534 - acc: 0.0000e+00 - val_loss: 177.9108 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 157.9114 - acc: 0.0000e+00 - val_loss: 137.5712 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "7654/7654 [==============================] - 1s 78us/sample - loss: 121.7896 - acc: 0.0000e+00 - val_loss: 107.8847 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "7654/7654 [==============================] - 0s 65us/sample - loss: 95.8038 - acc: 0.0000e+00 - val_loss: 85.7099 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "7654/7654 [==============================] - 0s 65us/sample - loss: 76.8009 - acc: 0.0000e+00 - val_loss: 68.8330 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "7654/7654 [==============================] - 1s 78us/sample - loss: 62.5735 - acc: 0.0000e+00 - val_loss: 56.2512 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "7654/7654 [==============================] - 1s 122us/sample - loss: 51.8143 - acc: 0.0000e+00 - val_loss: 46.8793 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "7654/7654 [==============================] - 1s 136us/sample - loss: 43.6965 - acc: 0.0000e+00 - val_loss: 39.7877 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "7654/7654 [==============================] - 1s 109us/sample - loss: 37.8684 - acc: 0.0000e+00 - val_loss: 34.4800 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "7654/7654 [==============================] - 1s 80us/sample - loss: 33.5159 - acc: 0.0000e+00 - val_loss: 30.7696 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "7654/7654 [==============================] - 1s 87us/sample - loss: 30.2690 - acc: 0.0000e+00 - val_loss: 27.9311 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "7654/7654 [==============================] - 1s 78us/sample - loss: 27.8978 - acc: 0.0000e+00 - val_loss: 25.9692 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "7654/7654 [==============================] - 1s 80us/sample - loss: 26.1435 - acc: 0.0000e+00 - val_loss: 24.5224 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "7654/7654 [==============================] - 1s 103us/sample - loss: 24.8969 - acc: 0.0000e+00 - val_loss: 23.4553 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "7654/7654 [==============================] - 1s 89us/sample - loss: 24.0290 - acc: 0.0000e+00 - val_loss: 22.6615 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "7654/7654 [==============================] - 1s 80us/sample - loss: 23.2962 - acc: 0.0000e+00 - val_loss: 22.1434 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "7654/7654 [==============================] - 1s 82us/sample - loss: 22.8390 - acc: 0.0000e+00 - val_loss: 21.8025 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "7654/7654 [==============================] - 1s 94us/sample - loss: 22.5472 - acc: 0.0000e+00 - val_loss: 21.3316 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "7654/7654 [==============================] - 1s 110us/sample - loss: 22.2638 - acc: 0.0000e+00 - val_loss: 21.3526 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "7654/7654 [==============================] - 1s 70us/sample - loss: 22.1441 - acc: 0.0000e+00 - val_loss: 20.9560 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "7654/7654 [==============================] - 1s 79us/sample - loss: 21.9476 - acc: 0.0000e+00 - val_loss: 20.7069 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "7654/7654 [==============================] - 1s 87us/sample - loss: 21.8020 - acc: 0.0000e+00 - val_loss: 21.0157 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "7654/7654 [==============================] - 1s 76us/sample - loss: 21.8579 - acc: 0.0000e+00 - val_loss: 20.4949 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "7654/7654 [==============================] - 1s 90us/sample - loss: 21.6294 - acc: 0.0000e+00 - val_loss: 20.3161 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "7654/7654 [==============================] - 1s 67us/sample - loss: 21.6444 - acc: 0.0000e+00 - val_loss: 20.3745 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "7654/7654 [==============================] - 1s 76us/sample - loss: 21.6261 - acc: 0.0000e+00 - val_loss: 20.3617 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "7654/7654 [==============================] - 1s 66us/sample - loss: 21.5677 - acc: 0.0000e+00 - val_loss: 20.6894 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "7654/7654 [==============================] - 1s 83us/sample - loss: 21.5173 - acc: 0.0000e+00 - val_loss: 20.3625 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "7654/7654 [==============================] - 1s 69us/sample - loss: 21.4714 - acc: 0.0000e+00 - val_loss: 20.2191 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "7654/7654 [==============================] - 1s 69us/sample - loss: 21.5686 - acc: 0.0000e+00 - val_loss: 20.2964 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "7654/7654 [==============================] - 1s 66us/sample - loss: 21.4969 - acc: 0.0000e+00 - val_loss: 20.3172 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 21.4312 - acc: 0.0000e+00 - val_loss: 20.0734 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "7654/7654 [==============================] - 1s 70us/sample - loss: 21.5017 - acc: 0.0000e+00 - val_loss: 20.0251 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "7654/7654 [==============================] - 1s 88us/sample - loss: 21.4389 - acc: 0.0000e+00 - val_loss: 19.9787 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "7654/7654 [==============================] - 1s 84us/sample - loss: 21.3221 - acc: 0.0000e+00 - val_loss: 20.3474 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "7654/7654 [==============================] - 1s 72us/sample - loss: 21.3746 - acc: 0.0000e+00 - val_loss: 20.2476 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "7654/7654 [==============================] - 1s 76us/sample - loss: 21.4378 - acc: 0.0000e+00 - val_loss: 19.9666 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "7654/7654 [==============================] - 1s 84us/sample - loss: 21.5596 - acc: 0.0000e+00 - val_loss: 19.9355 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 21.4104 - acc: 0.0000e+00 - val_loss: 20.1005 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "7654/7654 [==============================] - 1s 90us/sample - loss: 21.4130 - acc: 0.0000e+00 - val_loss: 20.0150 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "7654/7654 [==============================] - 1s 78us/sample - loss: 21.3931 - acc: 0.0000e+00 - val_loss: 21.0450 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "7654/7654 [==============================] - 1s 96us/sample - loss: 21.5298 - acc: 0.0000e+00 - val_loss: 19.9648 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "7654/7654 [==============================] - 1s 98us/sample - loss: 21.3838 - acc: 0.0000e+00 - val_loss: 20.3668 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "7654/7654 [==============================] - 1s 82us/sample - loss: 21.2419 - acc: 0.0000e+00 - val_loss: 20.0113 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "7654/7654 [==============================] - 1s 82us/sample - loss: 21.3345 - acc: 0.0000e+00 - val_loss: 20.7086 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "7654/7654 [==============================] - 1s 71us/sample - loss: 21.5513 - acc: 0.0000e+00 - val_loss: 20.4566 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "7654/7654 [==============================] - 1s 94us/sample - loss: 21.4352 - acc: 0.0000e+00 - val_loss: 19.8104 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "7654/7654 [==============================] - 1s 68us/sample - loss: 21.3776 - acc: 0.0000e+00 - val_loss: 19.9315 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "7654/7654 [==============================] - 1s 74us/sample - loss: 21.4797 - acc: 0.0000e+00 - val_loss: 19.8129 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "7654/7654 [==============================] - 1s 96us/sample - loss: 21.2991 - acc: 0.0000e+00 - val_loss: 20.0719 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "7654/7654 [==============================] - 1s 81us/sample - loss: 21.3531 - acc: 0.0000e+00 - val_loss: 20.1474 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "7654/7654 [==============================] - 1s 71us/sample - loss: 21.2424 - acc: 0.0000e+00 - val_loss: 19.7876 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "7654/7654 [==============================] - 1s 85us/sample - loss: 21.3579 - acc: 0.0000e+00 - val_loss: 20.2334 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "7654/7654 [==============================] - 1s 86us/sample - loss: 21.4245 - acc: 0.0000e+00 - val_loss: 19.7654 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "7654/7654 [==============================] - 1s 80us/sample - loss: 21.3657 - acc: 0.0000e+00 - val_loss: 19.8040 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "7654/7654 [==============================] - 1s 75us/sample - loss: 21.4060 - acc: 0.0000e+00 - val_loss: 20.0920 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "7654/7654 [==============================] - 1s 68us/sample - loss: 21.4284 - acc: 0.0000e+00 - val_loss: 20.0207 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "7654/7654 [==============================] - 1s 68us/sample - loss: 21.4342 - acc: 0.0000e+00 - val_loss: 19.8105 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "7654/7654 [==============================] - 1s 69us/sample - loss: 21.4016 - acc: 0.0000e+00 - val_loss: 20.0594 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "7654/7654 [==============================] - 1s 67us/sample - loss: 21.3510 - acc: 0.0000e+00 - val_loss: 19.7438 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "7654/7654 [==============================] - 1s 70us/sample - loss: 21.4033 - acc: 0.0000e+00 - val_loss: 19.7765 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "7654/7654 [==============================] - 1s 73us/sample - loss: 21.3093 - acc: 0.0000e+00 - val_loss: 19.8065 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "7654/7654 [==============================] - 1s 71us/sample - loss: 21.3174 - acc: 0.0000e+00 - val_loss: 19.7685 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "7654/7654 [==============================] - 1s 72us/sample - loss: 21.3116 - acc: 0.0000e+00 - val_loss: 19.9018 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "7654/7654 [==============================] - 1s 70us/sample - loss: 21.5507 - acc: 0.0000e+00 - val_loss: 19.8910 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "7654/7654 [==============================] - 1s 73us/sample - loss: 21.2802 - acc: 0.0000e+00 - val_loss: 19.9875 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "7654/7654 [==============================] - 1s 68us/sample - loss: 21.2402 - acc: 0.0000e+00 - val_loss: 19.8402 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "7654/7654 [==============================] - 1s 68us/sample - loss: 21.2723 - acc: 0.0000e+00 - val_loss: 19.7486 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "7654/7654 [==============================] - 1s 71us/sample - loss: 21.3481 - acc: 0.0000e+00 - val_loss: 19.8955 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "7654/7654 [==============================] - 1s 73us/sample - loss: 21.3545 - acc: 0.0000e+00 - val_loss: 20.3834 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "7654/7654 [==============================] - 1s 75us/sample - loss: 21.4677 - acc: 0.0000e+00 - val_loss: 19.8934 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "7654/7654 [==============================] - 1s 70us/sample - loss: 21.2365 - acc: 0.0000e+00 - val_loss: 19.7683 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "7654/7654 [==============================] - 1s 70us/sample - loss: 21.2887 - acc: 0.0000e+00 - val_loss: 20.2310 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "7654/7654 [==============================] - 1s 67us/sample - loss: 21.3142 - acc: 0.0000e+00 - val_loss: 19.9703 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "7654/7654 [==============================] - 1s 71us/sample - loss: 21.3087 - acc: 0.0000e+00 - val_loss: 20.1992 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "7654/7654 [==============================] - 1s 74us/sample - loss: 21.4191 - acc: 0.0000e+00 - val_loss: 19.8311 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "7654/7654 [==============================] - 1s 72us/sample - loss: 21.4736 - acc: 0.0000e+00 - val_loss: 20.0676 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "7654/7654 [==============================] - 1s 73us/sample - loss: 21.3948 - acc: 0.0000e+00 - val_loss: 19.9843 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "7654/7654 [==============================] - 1s 71us/sample - loss: 21.2777 - acc: 0.0000e+00 - val_loss: 20.0452 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1792188ddc8>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(x_train, y_train, epochs = 100, batch_size = 32, validation_data = [x_test, y_test]) #, callbacks = [early_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0H0zKKNEBLD5"
   },
   "source": [
    "### Predicting the results of the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "IA0yApEmBG1X",
    "outputId": "cb981e1f-9204-4a2a-fece-9d66a6919189"
   },
   "outputs": [],
   "source": [
    "y_pred = ann.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[431.33, 431.23],\n",
       "       [458.1 , 460.01],\n",
       "       [462.64, 461.14],\n",
       "       ...,\n",
       "       [468.96, 473.26],\n",
       "       [441.9 , 438.  ],\n",
       "       [461.42, 463.28]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision = 2)\n",
    "np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(ann.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204522.299860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201231.919491</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191954.577537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178638.190667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157561.623912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132507.881237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106086.272722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79330.197206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59667.114009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42471.262196</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>21.308696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.199193</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>21.419070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.831119</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>21.473556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.067552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>21.394763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.984290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>21.277708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.045185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  acc       val_loss  val_acc\n",
       "0   204522.299860  0.0  201231.919491      0.0\n",
       "1   191954.577537  0.0  178638.190667      0.0\n",
       "2   157561.623912  0.0  132507.881237      0.0\n",
       "3   106086.272722  0.0   79330.197206      0.0\n",
       "4    59667.114009  0.0   42471.262196      0.0\n",
       "..            ...  ...            ...      ...\n",
       "95      21.308696  0.0      20.199193      0.0\n",
       "96      21.419070  0.0      19.831119      0.0\n",
       "97      21.473556  0.0      20.067552      0.0\n",
       "98      21.394763  0.0      19.984290      0.0\n",
       "99      21.277708  0.0      20.045185      0.0\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17920309e88>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So no overfitting happened too! Also by using \"Early stopping\" the model went until epoch \"38/100\"!\n",
    "\n",
    "#Anyway, the goal in here (Regression in ANN) was to choose NOTHING for the output layer's \"activation\" and also to choose compile hyperparameter \"loss = 'mean_squared_error'\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Artificial Neural Network",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
